{"cells": [{"cell_type": "markdown", "id": "threaded-publication", "metadata": {}, "source": "# uga 8360 p1 "}, {"cell_type": "code", "execution_count": 2, "id": "resident-yugoslavia", "metadata": {}, "outputs": [], "source": "from pyspark import SparkContext, SparkConf"}, {"cell_type": "code", "execution_count": 3, "id": "active-enforcement", "metadata": {}, "outputs": [], "source": "import numpy as np"}, {"cell_type": "code", "execution_count": 4, "id": "suburban-stylus", "metadata": {}, "outputs": [], "source": "import math"}, {"cell_type": "markdown", "id": "under-bailey", "metadata": {}, "source": "# initial setup of the spark"}, {"cell_type": "code", "execution_count": 5, "id": "qualified-bullet", "metadata": {}, "outputs": [], "source": "conf = SparkConf().setAppName(\"FirstNotebook\")\nsc = SparkContext.getOrCreate(conf=conf)\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Word Count\") \\\n    .getOrCreate()"}, {"cell_type": "markdown", "id": "nervous-inflation", "metadata": {}, "source": "#  the learning nb based Dr. Quinn statement"}, {"cell_type": "code", "execution_count": 8, "id": "signed-stack", "metadata": {}, "outputs": [], "source": "# naive bayes training to compute\n\ndef naivebayeslearn():\n    #feature number and class number are fixed and hard coded\n    \n    #1. each class probability\n    bigp=[]\n    \n    #labels=sc.textFile(\"gs://uga-dsp/project1/files/y_train.txt\").map(lambda x:(x,1)).reduceByKey(lambda x, y: x + y).sortByKey().collect()\n    #total=sc.textFile(\"gs://uga-dsp/project1/files/y_train.txt\").count()\n    labels=sc.textFile(\"gs://uga-dsp/project1/files/y_small_train.txt\").map(lambda x:(x,1)).reduceByKey(lambda x, y: x + y).sortByKey().collect()\n    total=sc.textFile(\"gs://uga-dsp/project1/files/y_small_train.txt\").count()\n     \n    \n    \n    \n    \n    \n    \n     \n    \n    for dic in labels:\n        bigp.append(dic[1]/total)\n        \n        \n    #2. each word counts of one class C1 , and of all class C2    \n    featurep=np.ones((9,256))   #one could take care those 0 thing   \n    #labelInOrder=sc.textFile(\"gs://uga-dsp/project1/files/y_train.txt\").collect()    \n    labelInOrder=sc.textFile(\"gs://uga-dsp/project1/files/y_small_train.txt\").collect() \n    filenames=sc.textFile(\"gs://uga-dsp/project1/files/X_small_train.txt\").collect() \n    \n    \n    #filenames=sc.textFile(\"gs://uga-dsp/project1/files/X_train.txt\").collect()  \n    \n    commonpath=\"gs://uga-dsp/project1/data/bytes/\"\n    print('learning')\n    aggregate=sc.emptyRDD() \n\n    for classindex in range(9):\n        print(\"start \",classindex)\n        initial=0 \n        for file,indice in zip(filenames,labelInOrder):\n            if (int(indice)-1)==classindex:          \n                single=commonpath+file+'.bytes'\n                #print(single)\n                if initial==0:\n\n                    t1=sc.textFile(single).flatMap(lambda line: line.split()[1:]) #remove the pointer and split each feature\n                    assembly=t1.map(lambda x:(x,1)).reduceByKey(lambda x, y: x + y).sortByKey() #convert and aggregate\n                    #print('this initial',end='\\r')\n                    #print('this initial: ',single, assembly.first())\n                    initial=1\n                    #aggregate=t1\n                    aggregate=assembly\n                else:\n                    tw=sc.textFile(single).flatMap(lambda line: line.split()[1:]) #remove the pointer and split each feature\n                    #aggregate=tw.union(aggregate)\n                    #print(\"before combine\",aggregate.first())  \n                    #aggregate=aggregate.join(tw)\n                    \n                    assembly2=tw.map(lambda x:(x,1)).reduceByKey(lambda x, y: x + y).sortByKey() #convert and aggregate\n                    #print(\" the new \",assembly2.first())\n\n                    #print(aggregate.first()) \n                    #print(single)\n                    #aggregate=assembly2.union(aggregate)\n                    #aggregate=assembly2.join(aggregate)\n                    #rdd1=assembly2.zipWithIndex().map(lambda (val, key): (key,val))\n                    #rdd2=aggregate.zipWithIndex().map(lambda (val, key): (key,val))\n                    aggregate=aggregate.union(assembly2).reduceByKey(lambda x, y: x + y).sortByKey()\n                    #flatMap(lambda x,[(y,z)]:x,y+z)\n                    #print(\"after combine\",aggregate.first())    \n     \n                #aggregate=aggregate.map(lambda x:(x,1)).reduceByKey(lambda x, y: x + y).sortByKey()\n                index=0\n                #print(aggregate.first(), end='\\r')\n        for item in aggregate.collect():\n            if index<256:\n                featurep[classindex][index]+=item[1]\n\n                index+=1\n        print('done: ',classindex)\n        aggregate=sc.emptyRDD()         \n            #else:\n                #print('what is going on')\n                               \n        \n    #guess need to union all the same class numbers before assigning to a numpy array.    \n    return bigp,featurep"}, {"cell_type": "markdown", "id": "fresh-harassment", "metadata": {}, "source": "# run "}, {"cell_type": "code", "execution_count": null, "id": "devoted-seattle", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "learning\nstart  0\ndone:  0\nstart  1\ndone:  1\nstart  2\ndone:  2\nstart  3\ndone:  3\nstart  4\ndone:  4\nstart  5\ndone:  5\nstart  6\ndone:  6\nstart  7\ndone:  7\nstart  8\ndone:  8\n"}], "source": "b1,b2=naivebayeslearn()"}, {"cell_type": "code", "execution_count": 11, "id": "eleven-pierce", "metadata": {}, "outputs": [], "source": "b2/=np.sum(b2,axis=0)"}, {"cell_type": "code", "execution_count": null, "id": "developed-assist", "metadata": {}, "outputs": [], "source": "b1"}, {"cell_type": "code", "execution_count": 12, "id": "hazardous-independence", "metadata": {}, "outputs": [{"data": {"text/plain": "0.4933217794802869"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "b2[1][87]"}, {"cell_type": "markdown", "id": "respective-statement", "metadata": {}, "source": "# prediction"}, {"cell_type": "code", "execution_count": null, "id": "approximate-design", "metadata": {}, "outputs": [], "source": "def naivebayesprediction(b1,b2):\n    \n    prediction=[]\n    \n    commonpath=\"gs://uga-dsp/project1/data/bytes/\"\n    \n    #hard code the path\n    #\n    #for file in sc.textFile(\"gs://uga-dsp/project1/files/X_test.txt\").collect():\n    for file in sc.textFile(\"gs://uga-dsp/project1/files/X_small_test.txt\").collect():\n        single=commonpath+file+'.bytes'\n        t1=sc.textFile(single).flatMap(lambda line: line.split()[1:]) #remove the pointer and split each feature\n        assembly=t1.map(lambda x:(x,1)).reduceByKey(lambda x, y: x + y).sortByKey().collect() #convert and aggregate\n        possibility=np.zeros((9))\n        index=0\n        for zain in range(9):\n            possibility[zain]=math.log(b1[zain])\n            for item in assembly:\n                if index<256:\n                    possibility[zain]+=math.log(b2[zain][index])*item[1]\n                    index+=1\n                    \n        prediction.append(np.argmax(possibility)+1)\n        \n    \n    \n    return prediction"}, {"cell_type": "markdown", "id": "unable-street", "metadata": {}, "source": "# predict"}, {"cell_type": "code", "execution_count": null, "id": "historic-thursday", "metadata": {}, "outputs": [], "source": "result=naivebayesprediction(b1,b2)"}, {"cell_type": "code", "execution_count": 17, "id": "welcome-marsh", "metadata": {}, "outputs": [{"data": {"text/plain": "[3, 3]"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "result[7:9]"}, {"cell_type": "code", "execution_count": 18, "id": "australian-abortion", "metadata": {}, "outputs": [], "source": "groundtruth=sc.textFile(\"gs://uga-dsp/project1/files/y_small_train.txt\").collect() "}, {"cell_type": "code", "execution_count": 23, "id": "fatty-instrument", "metadata": {}, "outputs": [{"data": {"text/plain": "'6'"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "groundtruth[0]"}, {"cell_type": "code", "execution_count": 24, "id": "balanced-professional", "metadata": {}, "outputs": [{"data": {"text/plain": "3"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "result[0]"}, {"cell_type": "code", "execution_count": 25, "id": "centered-campaign", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "accuracy:  0.12137203166226913\n"}], "source": "bingo=0\nfor k,j in zip(groundtruth,result):\n    if int(k)==j:\n        bingo+=1\n#bingo=groundtruth==result\nprint(\"accuracy: \", bingo/len(groundtruth))\n"}, {"cell_type": "markdown", "id": "optional-elements", "metadata": {}, "source": "# serialize"}, {"cell_type": "code", "execution_count": null, "id": "satisfied-setup", "metadata": {}, "outputs": [], "source": "with open('gs://dsp-pj1/y_test.txt', 'w') as f:\n    for item in result:\n        \n        f.write(\"%s\\n\" % (item+1))"}, {"cell_type": "code", "execution_count": null, "id": "devoted-compiler", "metadata": {}, "outputs": [], "source": "with open('y_test.txt', 'w') as f:\n    for item in result:\n        it=item+1\n        f.write(\"%s\\n\" % it)"}, {"cell_type": "code", "execution_count": null, "id": "jewish-assumption", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}, "toc-autonumbering": true, "toc-showmarkdowntxt": true}, "nbformat": 4, "nbformat_minor": 5}